# SSCNet 2017
## 重要性 创新点

1. 提出了SSC问题，combine occupancy prediction and semantic labels prediction
2. 

## 具体方法

1. input:
- single depth image
2. module
- dilation-based 3D context module -> expand receptive field
- SUNGG: large-scale synthetic 3D scene dataset -> train
3. ouput: simultaneously outputs occupancy and semantic labels for all voxels in the camera view frustrum
4. SSCNet:
- depth map -> 3D volume -> 3D conv (extract and aggregate both local geometric and contextual information) -> probability distribution of voxel occupancy and object categories
- depth map -> TSDF (Truncated Signed Distance Function)
  - represent 2D observation into same 3D physical space as output
  - invariant to the viewpoint projection
  - meaningful for the network to learn geometry and scene representation
  - flipped TSDF
    - eliminate view dependency
    - eliminate strong gradients in empty space -> strong gradient on surface
- network architecture
  - high-resolution 3D volume as input
  - several 3D conv layers: local geometry representation
  - dilation-based 3D context module: higher-level inter-object contextual information
  - 2 more conv layers: concatenate and aggregate information from multi scales
  - voxel-wise softmax layer: predict final voxel label

## 计算速度
1. pre-train SSCNet on SUNCG: a week on Tesla K40 GPU
2. fine-tune on NYU dataset: 30 hours
3. 11GB GPU memory

## 实验结果

## Limitations
1. without color information
2. output resolution is lower than input volume
