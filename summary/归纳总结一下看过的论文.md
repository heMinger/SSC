# SSCNet 2017
## 重要性 创新点

1. 提出了SSC问题，combine occupancy prediction and semantic labels prediction
2. end-to-end convolutional network

## 具体方法

1. input:
- single depth image
2. module
- dilation-based 3D context module -> expand receptive field
- SUNGG: large-scale synthetic 3D scene dataset -> train
3. ouput: simultaneously outputs occupancy and semantic labels for all voxels in the camera view frustrum
4. SSCNet:
- depth map -> 3D volume -> 3D conv (extract and aggregate both local geometric and contextual information) -> probability distribution of voxel occupancy and object categories
- depth map -> TSDF (Truncated Signed Distance Function)
  - represent 2D observation into same 3D physical space as output
  - invariant to the viewpoint projection
  - meaningful for the network to learn geometry and scene representation
  - flipped TSDF
    - eliminate view dependency
    - eliminate strong gradients in empty space -> strong gradient on surface
- network architecture
  - high-resolution 3D volume as input
  - several 3D conv layers: local geometry representation
  - dilation-based 3D context module: higher-level inter-object contextual information
  - 2 more conv layers: concatenate and aggregate information from multi scales
  - voxel-wise softmax layer: predict final voxel label
 

## 计算速度
1. pre-train SSCNet on SUNCG: a week on Tesla K40 GPU
2. fine-tune on NYU dataset: 30 hours
3. 11GB GPU memory

## 实验结果

## Limitations
1. without color information
2. output resolution is lower than input volume

# LMSCNet: Lightweight Multiscale 3D Semantic Completion

## 题目中重点

- Lightweight
- Multiscale

## 主体设计
1. LiDAR scans -> sparse -> allievate heavy computation + low resolution
2. 2D Unet backbone + 3D segmentation heads
3. 大致流程：
- LiDAR scans -> encoded as voxel
- 2D convolution -> process 3D voxel grid
  - by convolving along one spatial axis
- 将2D 特征转回3D空间，使用3D segmentation head进行分割
4. multiscale completion -> reduce inference time

## 重点
1. 包含2D feature转到3D的操作
- **$\color{red}{\text{In other words, while 2D CNNs output 3D features maps, our decoder must output 4D tensor; the last dimension being the semantic class-wise probability distribution.}}$**
- 
2. 



## 对比
1. 卷积方式：
- SSCNet： 直接使用3D conv (dilated-based) 对voxel grid进行处理
- LMSCNet： 使用2D卷积（x-y plane -> 扩展到3D，使用3D segmentation head 进行预测
  - 也是挺有想法，就算输入的是3D数据也用2D卷积进行处理
  - 最后转回3D空间再用3D进行segment
2. $\color{red}{虽然输入的数据不一样，但都进行了体素化，用voxel grid进行表示}$
3. 
